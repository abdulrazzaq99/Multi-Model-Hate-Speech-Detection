{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GLqsmYTq1v-X"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wz-RNvk11v-Z",
        "outputId": "9b86e1b0-67f4-42be-aa23-561597753d41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Download stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Load dataset and take only 2 lakh entries\n",
        "df = pd.read_csv('HateSpeechDatasetBalanced.csv').sample(n=100000, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "vpdtWDp41v-a"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "# Separate majority and minority classes\n",
        "df_majority = df[df.Label == 1]\n",
        "df_minority = df[df.Label == 0]\n",
        "\n",
        "# Check the number of samples in the minority class\n",
        "n_minority_samples = len(df_minority)\n",
        "\n",
        "# Downsample majority class\n",
        "df_majority_downsampled = resample(df_majority,\n",
        "                                   replace=False,    # sample without replacement\n",
        "                                   n_samples=min(len(df_majority), n_minority_samples),     # to match minority class\n",
        "                                   random_state=42) # reproducible results\n",
        "\n",
        "# Combine minority class with downsampled majority class\n",
        "df_balanced = pd.concat([df_majority_downsampled, df_minority])\n",
        "\n",
        "# Shuffle the dataset\n",
        "df_balanced = df_balanced.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Update the dataframe\n",
        "df = df_balanced"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-u-ro_2Z1v-a"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "\n",
        "# Preprocess text data\n",
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
        "    text = re.sub(r'\\s+', ' ', text)  # Remove double spaces\n",
        "    text = ''.join([char for char in text if char not in string.punctuation])\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text\n",
        "\n",
        "df['Content'] = df['Content'].apply(preprocess_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "TOK1FyyP1v-a"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "\n",
        "# Remove special characters and punctuation\n",
        "df['Content'] = df['Content'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x))\n",
        "\n",
        "# Convert all words to lowercase\n",
        "df['Content'] = df['Content'].apply(lambda x: x.lower())\n",
        "\n",
        "# Counter Vectorizer\n",
        "count_vectorizer = CountVectorizer()\n",
        "X_count = count_vectorizer.fit_transform(df['Content'])\n",
        "\n",
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "X_tfidf = tfidf_vectorizer.fit_transform(df['Content'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "L-udUd1x1v-b"
      },
      "outputs": [],
      "source": [
        "# Define features and target variable\n",
        "X = X_tfidf\n",
        "y = df['Label']\n",
        "\n",
        "# Split the dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "z2wZfeh21v-b"
      },
      "outputs": [],
      "source": [
        "# Initialize models\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'SVM': SVC(),\n",
        "    'AdaBoost': AdaBoostClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Naive Bayes': MultinomialNB(),\n",
        "    'Decision Tree': DecisionTreeClassifier()\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4_q0y3e1v-b",
        "outputId": "2893bf18-1a3b-4555-d629-590516563e65"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Model  Accuracy\n",
            "0  Logistic Regression   0.67975\n",
            "1        Random Forest   0.75525\n",
            "2                  SVM   0.71700\n",
            "3             AdaBoost   0.67175\n",
            "4                  KNN   0.50225\n",
            "5          Naive Bayes   0.75725\n",
            "6        Decision Tree   0.71750\n"
          ]
        }
      ],
      "source": [
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from scipy import sparse\n",
        "\n",
        "# Assuming X_train and X_test are sparse matrices\n",
        "scaler = StandardScaler(with_mean=False)  # with_mean=False is necessary for sparse matrices\n",
        "\n",
        "# Scale the data\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train and evaluate models\n",
        "accuracy_scores = {}\n",
        "for model_name, model in models.items():\n",
        "    if model_name == 'Naive Bayes':\n",
        "        nb_scaler = MinMaxScaler()\n",
        "        X_train_nb_scaled = nb_scaler.fit_transform(X_train.toarray())\n",
        "        X_test_nb_scaled = nb_scaler.transform(X_test.toarray())\n",
        "        model.fit(X_train_nb_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_nb_scaled)\n",
        "    else:\n",
        "        if model_name == 'Logistic Regression':\n",
        "            model.set_params(max_iter=1000)  # Increase max_iter for Logistic Regression\n",
        "        if model_name == 'AdaBoost':\n",
        "            model.set_params(algorithm='SAMME')  # Use SAMME algorithm for AdaBoost\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        y_pred = model.predict(X_test_scaled)\n",
        "    # Collect accuracy scores\n",
        "    accuracy_scores[model_name] = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Create a DataFrame to display the results\n",
        "accuracy_df = pd.DataFrame(list(accuracy_scores.items()), columns=['Model', 'Accuracy'])\n",
        "print(accuracy_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "60ZLaMfw1v-c"
      },
      "outputs": [],
      "source": [
        "# Multi-Modal Ensemble (Hard Voting)\n",
        "ensemble_model = VotingClassifier(estimators=[\n",
        "    ('lr', LogisticRegression()),\n",
        "    ('rf', RandomForestClassifier()),\n",
        "    ('svm', SVC()),\n",
        "    ('ada', AdaBoostClassifier()),\n",
        "    ('knn', KNeighborsClassifier()),\n",
        "    ('nb', MultinomialNB()),\n",
        "    ('dt', DecisionTreeClassifier())\n",
        "], voting='hard')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RquBUtQu1v-c",
        "outputId": "2036d146-3c6e-43a6-c734-aa468d28edaf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/ensemble/_weight_boosting.py:527: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ensemble Model Accuracy: 0.7535\n"
          ]
        }
      ],
      "source": [
        "ensemble_model.fit(X_train_scaled, y_train)\n",
        "y_pred_ensemble = ensemble_model.predict(X_test_scaled)\n",
        "print(f'Ensemble Model Accuracy: {accuracy_score(y_test, y_pred_ensemble)}')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8ezc9dC9AxC8"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}